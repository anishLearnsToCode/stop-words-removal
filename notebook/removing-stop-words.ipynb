{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words\n",
    "## Anish Sachdeva (DTU/2K16/MC/013)\n",
    "## Natural Language Processing - Dr. Seba Susan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the NLTK Library and Functions\n",
    "We need to import the [nltk](https://www.nltk.org/) libray which is heavily used in Natural language Preprocessing and we will import some specific functions and classes from the library that will help us with our data preprocessing. \n",
    "\n",
    "> Note: If the following imports do not function, most likely one or more library is not installed on your machine. Oen your terminal and run the `pip install nltk` and `pip install pickle` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Corpus\n",
    "We will use standard Python functions to read our __resume.txt__ file which has some textual data and is my current resume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resume file has the following data\n",
      "Anish Sachdeva\n",
      "Software Developer + Clean Code Enthusiast\n",
      "\n",
      "Phone : 8287428181\n",
      "email : anish_@outlook.com\n",
      "home : sandesh vihar, pitampura, new delhi - 110034\n",
      "date of birth : 7th April 1998\n",
      "languages : English, Hindi, French\n",
      "\n",
      "Work Experience\n",
      "What After College (4 months)\n",
      "Delhi, India\n",
      "Creating content to teach Core Java and Python with Data Structures and Algorithms and giving online classes to students\n",
      "\n",
      "Summer Research Fellow at University of Auckland (2 Months)\n",
      "Auckland, New Zealand\n",
      "Worked on Geometry of Mobius Transformations, Differential Grometry under Dr. Pedram Hekmati at the Department of Mathematics, University of Auckland\n",
      "\n",
      "Software Developer at CERN (14 Months)\n",
      "CERN, Geneva, Switzerland\n",
      "Worked in the core Platforms team of the FAP-BC group. Part of an agile team of developers that maintains and adds core functionality to applications used internally at CERN by HR, Financial, Administrative and other departments including Scientific\n",
      "Worked on legacy applications that comprise of single and some times multiple frameworks such as Java Spring, Boot, Hibernate and Java EE. Also worked with Google Polymer 1.0 and JSP on the client side\n",
      "Maintained CERN's Electronic Document Handing System application with >1M LOC that comprising of multiple frameworks and created ~20 years ago. Worked on feature requests, support requests and incidents and also release cycles\n",
      "\n",
      "Teaching Assistant (4 Months)\n",
      "Coding Ninjas, Delhi\n",
      "Served as the teaching assistant to Nucleus - Java with DS batch, under Mr. Ankur Kumar. Worked on creating course content and quizzes for online platform of Coding Ninjas for Java. Helped students in core Data Structures and Algorithms concepts in Java\n",
      "\n",
      "Education\n",
      "Delhi Technological University (2016 - 2021)\n",
      "Bachelors of Technology Mathematics and Computing\n",
      "CGPA: 9.2\n",
      "\n",
      "The Heritage School Rohini (2004 - 2016)\n",
      "Physics, Chemistry, Maths + Computer Science with English\n",
      "Senior Secondary: 94.8%\n",
      "Secondary: 9.8 CGPA\n",
      "\n",
      "Technical Skills\n",
      "Java + Algorithms and Data Structures\n",
      "MEAN Stack Web Development\n",
      "Python + Machine Learning\n",
      "MATLAB + Octave\n",
      "MySQL, PostgresSQL & MongoDB\n",
      "\n",
      "Other Skills\n",
      "MS Office, Adobe Photoshop, LaTeX + MiTeX\n",
      "\n",
      "University Courses\n",
      "Applied Mathematics I, II, III\n",
      "Linear Algebra + Probability & Statistics + Stochastic Processes + Discrete Maths\n",
      "Computer Organization & Architecture + Data Structures + Algorithm Design and Analysis + DBMS + OS\n",
      "Computer Vision + NLP\n",
      "\n",
      "Important Links\n",
      "https://www.linkedin.com/in/anishsachdeva1998/\n",
      "https://github.com/anishLearnsToCode\n",
      "https://www.hackerrank.com/anishviewer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read()\n",
    "resume_file.close()\n",
    "print('The resume file has the following data')\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Resume File\n",
    "There are 3 ways to tokenize the resume corpus we have opened.\n",
    "1. We create our own function that uses the python regex __re__ library and iterate over our file to extract tokens.\n",
    "1. The second way is to use the inbuilt `word_tokenize` method in the `nltk.tokenize` package.\n",
    "1. The third way is we craete our tokenizer usin the 'nltk.RegexTokenizer' factory method. \n",
    "\n",
    "> Note: here the term token refers to a word. In NLP jargon, a word is referred to as a token and in the following notebook and in NLP in general the 2 are used interchangibly.\n",
    "\n",
    "We will impliment all 3 and see their different advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(document):\n",
    "    tokens = []\n",
    "    for sentence in document.split('\\n'):\n",
    "        for word in sentence.split():\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'message', 'string.', 'Will', 'this', 'work?']\n"
     ]
    }
   ],
   "source": [
    "# Testing our Function\n",
    "print(custom_tokenizer('this is a simple message string. Will this work?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above example that our function works well but is making a mistake with recognizing tokens when attached with punctutions. Words such as __work?__ should appear as `['work', '?']` as seperate tokens but the python `split()` method is unable to seperate tokens from punctuations.\n",
    "\n",
    "The above code can further be shortened using the inbuilt `word_tokenize()` function defined inside `nltk.tokenize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using the `word_tokenize` Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'message', 'string', '.', 'Will', 'this', 'work', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize('this is a simple message string. Will this work?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that this is a cleaner API and this performed much better than our custom function as this was able to seperate out punctuations from the tokens. But in our token extraction step from our corpuswe also need to remove punctuations andto accomplish that we create our own tokenizer using the `nltk.RegexTokenizer` factory Method.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'soo', 'cool', 'üòé', 'let', \"'s\", 'celebrate']\n"
     ]
    }
   ],
   "source": [
    "# Note: Emojis ae also treated as a single token and are retained in tokenization process using the word_tokenize function\n",
    "print(word_tokenize(\"this is soo cool üòé let's celebrate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using the `nltk.RegexTokenizer` Factory Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'simple', 'message', 'string', 'Will', 'this', 'work']\n"
     ]
    }
   ],
   "source": [
    "# testing the tokenizer (you can also test any string of your choice here)\n",
    "print(tokenizer.tokenize('this is a simple message string. Will this work?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all punctuations have gone and we can use this to tokenize our resume which will remove punctuations and also give us tokens in the original case. \n",
    "\n",
    "> Note: The Tokenizer doesn't change the case of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Tokenizing the Resume\n",
    "Before tokenizing we must convert our text data to lowercase and then tokenize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = resume.lower()\n",
    "# Using the nltk.RegexTokenizerbuilt before\n",
    "tokens = tokenizer.tokenize(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens are: 363\n",
      "The first 40 tokens are: ['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', '8287428181', 'email', 'anish_', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', '110034', 'date', 'of', 'birth', '7th', 'april', '1998', 'languages', 'english', 'hindi', 'french', 'work', 'experience', 'what', 'after', 'college', '4', 'months', 'delhi', 'india', 'creating']\n"
     ]
    }
   ],
   "source": [
    "print('The number of tokens are:', len(tokens))\n",
    "print('The first 40 tokens are:', tokens[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only alphanumerical tokens have been retained and all other punctuation tokens like __.__ and __,__ have been removed. This step is very beneficial as punctautions in this particular context where we wish to extract the important information from text/resume or if we wish to run sentiment analysis do not contribute a lot to the information or the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing', 'news', 'this', 'is', 'sooo', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# Note: Emojis are also removed using the tokenizer we have built\n",
    "print(tokenizer.tokenize('amazing news üéâ this is sooo awesome üòÄüç∞'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing Stopwords From the Tokens\n",
    "Stopwords are a few special words in a language that are used in the language for a grammatical purpose, but from an Informatoon point of view do not add a lot of meaning to the corpus. These words can be removed without hampering information extraction or sentiment analysis. Some examples of these words are `['i', 'me', 'my', 'myself', 'yours', ....]` and many more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stopwords from the nltk corpus \n",
    "stopwords_en = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 20 stopwords are: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# print first 20 stopwords\n",
    "print('The first 20 stopwords are:', stopwords_en[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of stopwords in the english language as defined by nltk are: 179\n"
     ]
    }
   ],
   "source": [
    "print('The total number of stopwords in the english language as defined by nltk are:', len(stopwords_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert the stopwords into a set so that we can perform operations to check whether a word is a stopword or not in constant time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# testig whether a word is a stop word or not - you can modify this to test for any word\n",
    "print('i' in stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens after removing stopwords: 288\n",
      "First 20 tokens: ['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', '8287428181', 'email', 'anish_', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', '110034']\n"
     ]
    }
   ],
   "source": [
    "# Creating the updated tokens list after removing all stopwords \n",
    "stop_words_removed_tokens = [token for token in tokens if token not in stopwords_en]\n",
    "print('The number of tokens after removing stopwords:', len(stop_words_removed_tokens))\n",
    "print('First 20 tokens:', stop_words_removed_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 tokens (with stopwords) ['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', '8287428181', 'email', 'anish_', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', '110034', 'date', 'of', 'birth', '7th', 'april', '1998', 'languages', 'english', 'hindi', 'french'] \n",
      "\n",
      "First 30 tokens (without stopwords) ['anish', 'sachdeva', 'software', 'developer', 'clean', 'code', 'enthusiast', 'phone', '8287428181', 'email', 'anish_', 'outlook', 'com', 'home', 'sandesh', 'vihar', 'pitampura', 'new', 'delhi', '110034', 'date', 'birth', '7th', 'april', '1998', 'languages', 'english', 'hindi', 'french', 'work']\n"
     ]
    }
   ],
   "source": [
    "print('First 30 tokens (with stopwords)', tokens[:30], '\\n')\n",
    "print('First 30 tokens (without stopwords)', stop_words_removed_tokens[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing a difference in tokens in the above cell is tough, but we can compare the number of tokens to clearly see that after removing stopwords there has been a clear reduction in the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens (with stopwords): 363\n",
      "number of tokens (without stopwords): 288\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens (with stopwords):', len(tokens))\n",
    "print('number of tokens (without stopwords):', len(stop_words_removed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combing All Steps To Create a Pipeline\n",
    "We can now create a single python function that will take in a file location as the input and return a tokenized version in lowercase with punctuations and stopwords removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tokenize a given string document\n",
    "def tokenize(document):\n",
    "    document = document.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    tokens = [token for token in tokenizer.tokenize(document) if token not in stopwords_en]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tokenize a given file given a file path\n",
    "def tokenize_file(file_location):\n",
    "    file = open(file_location, 'r')\n",
    "    document = file.read()\n",
    "    file.close()\n",
    "    return tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'gentle', 'good', 'night', 'old', 'age', 'burn', 'rave', 'close', 'day', 'rage', 'rage', 'dying', 'light', 'though', 'wise', 'men', 'end', 'know', 'dark', 'right', 'words', 'forked', 'lightning', 'go', 'gentle', 'good', 'night', 'good', 'men', 'last', 'wave', 'crying', 'bright', 'frail', 'deeds', 'might', 'danced', 'green', 'bay', 'rage', 'rage', 'dying', 'light', 'wild', 'men', 'caught', 'sang', 'sun', 'flight', 'learn', 'late', 'grieved', 'way', 'go', 'gentle', 'good', 'night', 'grave', 'men', 'near', 'death', 'see', 'blinding', 'sight', 'blind', 'eyes', 'could', 'blaze', 'like', 'meteors', 'gay', 'rage', 'rage', 'dying', 'light', 'father', 'sad', 'height', 'curse', 'bless', 'fierce', 'tears', 'pray', 'go', 'gentle', 'good', 'night', 'rage', 'rage', 'dying', 'light']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_file('../assets/do-not-go-gentle-into-that-good-night.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question']\n"
     ]
    }
   ],
   "source": [
    "# sentence consiting of many stopwords\n",
    "print(tokenize('To be, or not to be. That is the question.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can finally call the tokenizer method on our resume file and save the tokens in a pickle file for further \n",
    "# analytics and discussion\n",
    "resume_tokens = tokenize_file('../assets/resume.txt')\n",
    "pickle.dump(resume_tokens, open('../assets/resume.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analytics\n",
    "We will create perform the following analytis on our document:\n",
    "1. To calculate unique words in a document.\n",
    "1. To calculate frequency of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokens from pickle file\n",
    "resume_tokens = pickle.load(open('../assets/resume.p', 'rb'))\n",
    "unique_tokens = set(resume_tokens)\n",
    "frequency = Counter(resume_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal Number of Tokens: 288\n",
      "Number of Unique Tokens: 214 \n",
      "\n",
      "Frequency of 15 most common tokens: \n",
      " [('java', 7), ('worked', 6), ('com', 4), ('delhi', 4), ('months', 4), ('core', 4), ('data', 4), ('structures', 4), ('university', 4), ('cern', 4), ('algorithms', 3), ('auckland', 3), ('mathematics', 3), ('computer', 3), ('https', 3)]\n"
     ]
    }
   ],
   "source": [
    "print('Totoal Number of Tokens:', len(resume_tokens))\n",
    "print('Number of Unique Tokens:', len(unique_tokens), '\\n')\n",
    "print('Frequency of 15 most common tokens: \\n', frequency.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running frequency analysis we see that the top skills + experiences that I have listed on my resume are Java, Data Structures and Algorithms, Mathematics, core computers and also university of auckland and CERN. These are all important informations that I would like prospective employers to know and thsi wouldn't have been possible if we wouldn't have removed stopwords and the top frequency words would've been 'I', 'am', 'doing', 'working', 'me' etc.\n",
    "\n",
    "Although the result isn't perfect. We see that __worked__ has also appeared in our tokens which doesn't convey a lot of information. We can be prety sure that people who have listed some experience on their resume have defininately _worked_ or _interned_ at that company, so for this particular task we can update our list of stopwords.\n",
    "\n",
    "Ideally we should curate our list of stopwords based on the domain we are working on. We will now add some Resume/CV related domain words to the stopwords set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en.add('work')\n",
    "stopwords_en.add('working')\n",
    "stopwords_en.add('worked')\n",
    "stopwords_en.add('intern')\n",
    "stopwords_en.add('interning')\n",
    "stopwords_en.add('interned')\n",
    "stopwords_en.add('https')\n",
    "stopwords_en.add('com')\n",
    "stopwords_en.add('new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we once again create our tokens and run anlytics\n",
    "# We can finally call the tokenizer method on our resume file and save the tokens in a pickle file for further \n",
    "# analytics and discussion\n",
    "resume_tokens = tokenize_file('../assets/resume.txt')\n",
    "pickle.dump(resume_tokens, open('../assets/resume.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal Number of Tokens: 272\n",
      "Number of Unique Tokens: 209 \n",
      "\n",
      "Frequency of 15 most common tokens: \n",
      " [('java', 7), ('delhi', 4), ('months', 4), ('core', 4), ('data', 4), ('structures', 4), ('university', 4), ('cern', 4), ('algorithms', 3), ('auckland', 3), ('mathematics', 3), ('computer', 3), ('software', 2), ('developer', 2), ('english', 2)]\n"
     ]
    }
   ],
   "source": [
    "# loading tokens from pickle file\n",
    "resume_tokens = pickle.load(open('../assets/resume.p', 'rb'))\n",
    "unique_tokens = set(resume_tokens)\n",
    "frequency = Counter(resume_tokens)\n",
    "print('Totoal Number of Tokens:', len(resume_tokens))\n",
    "print('Number of Unique Tokens:', len(unique_tokens), '\\n')\n",
    "print('Frequency of 15 most common tokens: \\n', frequency.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now infer more skills and experiences using the frequency analysis after added Resume/CV based (domain specific) stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Formatted (Pretty) Output\n",
    "We will create a utility function that will return the original document in a similaryl formatted manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_formatted(document):\n",
    "    result = []\n",
    "    for line in document.split('\\n'):\n",
    "        tokens = tokenize(line)\n",
    "        result.append(' '.join(tokens))\n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer clean code enthusiast\n",
      "\n",
      "phone 8287428181\n",
      "email anish_ outlook\n",
      "home sandesh vihar pitampura delhi 110034\n",
      "date birth 7th april 1998\n",
      "languages english hindi french\n",
      "\n",
      "experience\n",
      "college 4 months\n",
      "delhi india\n",
      "creating content teach core java python data structures algorithms giving online classes students\n",
      "\n",
      "summer research fellow university auckland 2 months\n",
      "auckland zealand\n",
      "geometry mobius transformations differential grometry dr pedram hekmati department mathematics university auckland\n",
      "\n",
      "software developer cern 14 months\n",
      "cern geneva switzerland\n",
      "core platforms team fap bc group part agile team developers maintains adds core functionality applications used internally cern hr financial administrative departments including scientific\n",
      "legacy applications comprise single times multiple frameworks java spring boot hibernate java ee also google polymer 1 0 jsp client side\n",
      "maintained cern electronic document handing system application 1m loc comprising multiple frameworks created 20 years ago feature requests support requests incidents also release cycles\n",
      "\n",
      "teaching assistant 4 months\n",
      "coding ninjas delhi\n",
      "served teaching assistant nucleus java ds batch mr ankur kumar creating course content quizzes online platform coding ninjas java helped students core data structures algorithms concepts java\n",
      "\n",
      "education\n",
      "delhi technological university 2016 2021\n",
      "bachelors technology mathematics computing\n",
      "cgpa 9 2\n",
      "\n",
      "heritage school rohini 2004 2016\n",
      "physics chemistry maths computer science english\n",
      "senior secondary 94 8\n",
      "secondary 9 8 cgpa\n",
      "\n",
      "technical skills\n",
      "java algorithms data structures\n",
      "mean stack web development\n",
      "python machine learning\n",
      "matlab octave\n",
      "mysql postgressql mongodb\n",
      "\n",
      "skills\n",
      "ms office adobe photoshop latex mitex\n",
      "\n",
      "university courses\n",
      "applied mathematics ii iii\n",
      "linear algebra probability statistics stochastic processes discrete maths\n",
      "computer organization architecture data structures algorithm design analysis dbms os\n",
      "computer vision nlp\n",
      "\n",
      "important links\n",
      "www linkedin anishsachdeva1998\n",
      "github anishlearnstocode\n",
      "www hackerrank anishviewer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resume_file = open('../assets/resume.txt', 'r')\n",
    "resume = resume_file.read()\n",
    "resume_file.close()\n",
    "resume_tokenized_pretty = tokenized_formatted(resume)\n",
    "print(resume_tokenized_pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now store this output in a text file\n",
    "pickle.dump(resume_tokenized_pretty, open('../assets/resume_tokenized.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anish sachdeva\n",
      "software developer clean code enthusiast\n",
      "\n",
      "phone 8287428181\n",
      "email anish_ outlook\n",
      "home sandesh vihar pitampura delhi 110034\n",
      "date birth 7th april 1998\n",
      "languages english hindi french\n",
      "\n",
      "experience\n",
      "college 4 months\n",
      "delhi india\n",
      "creating content teach core java python data structures algorithms giving online classes students\n",
      "\n",
      "summer research fellow university auckland 2 months\n",
      "auckland zealand\n",
      "geometry mobius transformations differential grometry dr pedram hekmati department mathematics university auckland\n",
      "\n",
      "software developer cern 14 months\n",
      "cern geneva switzerland\n",
      "core platforms team fap bc group part agile team developers maintains adds core functionality applications used internally cern hr financial administrative departments including scientific\n",
      "legacy applications comprise single times multiple frameworks java spring boot hibernate java ee also google polymer 1 0 jsp client side\n",
      "maintained cern electronic document handing system application 1m loc comprising multiple frameworks created 20 years ago feature requests support requests incidents also release cycles\n",
      "\n",
      "teaching assistant 4 months\n",
      "coding ninjas delhi\n",
      "served teaching assistant nucleus java ds batch mr ankur kumar creating course content quizzes online platform coding ninjas java helped students core data structures algorithms concepts java\n",
      "\n",
      "education\n",
      "delhi technological university 2016 2021\n",
      "bachelors technology mathematics computing\n",
      "cgpa 9 2\n",
      "\n",
      "heritage school rohini 2004 2016\n",
      "physics chemistry maths computer science english\n",
      "senior secondary 94 8\n",
      "secondary 9 8 cgpa\n",
      "\n",
      "technical skills\n",
      "java algorithms data structures\n",
      "mean stack web development\n",
      "python machine learning\n",
      "matlab octave\n",
      "mysql postgressql mongodb\n",
      "\n",
      "skills\n",
      "ms office adobe photoshop latex mitex\n",
      "\n",
      "university courses\n",
      "applied mathematics ii iii\n",
      "linear algebra probability statistics stochastic processes discrete maths\n",
      "computer organization architecture data structures algorithm design analysis dbms os\n",
      "computer vision nlp\n",
      "\n",
      "important links\n",
      "www linkedin anishsachdeva1998\n",
      "github anishlearnstocode\n",
      "www hackerrank anishviewer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can view contents of the file using the below code\n",
    "file = pickle.load(open('../assets/resume_tokenized.p', 'rb'))\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion\n",
    "We have seen that after removing the stopwords from the resume our number of words has gone down considerably and also that the words we removed never added a lot of meaning to our text. Large companies who will receive many resumes will want to search them using keywords such as `Java`, `Python`, `web Development` etc. and words such as `i`, `me`, `mine` are superfluous in nature.\n",
    "\n",
    "So, by removing these stopwords we have actually make our corpus more information dense and any other further task we might perform such as converting these words into embeddings or any other Machine Learning/Deep Learning task will now be done on a smaller Corpus and hence would run faster.\n",
    "\n",
    "Considering these above advantages removing stopwords is a very beneficial pre-processing step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
